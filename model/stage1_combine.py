# To run this code you need to install the following dependencies:
# pip install google-genai

import os
from google import genai
from google.genai import types
from google.genai.types import Tool, GenerateContentConfig
from dotenv import load_dotenv
load_dotenv()
import json
from ddgs import DDGS
from db.database import *
from fastapi import Depends
import re
import json


def search_with_duckduckgo(location):
    # ğŸ” Debug: å…ˆå°å‡ºä¾†çœ‹çœ‹ï¼Œç¢ºå®šçœŸçš„æœ‰å‚³å°é—œéµå­—é€²å»
    print(f"ğŸ•µï¸ æ­£åœ¨å‘ DuckDuckGo æŸ¥è©¢é—œéµå­—ï¼š[{location}]") 
    
    results = []
    urls = []
    try:
        with DDGS() as ddgs:
            # ==========================================
            # ğŸ›¡ï¸ åŠ å…¥ safesearch='strict' (åš´æ ¼éæ¿¾æˆäºº/åƒåœ¾å…§å®¹)
            # ğŸ›¡ï¸ ç¢ºèª region='tw-tz' (é–å®šå°ç£ç¹é«”ä¸­æ–‡çµæœ)
            # ==========================================
            ddgs_gen = ddgs.text(
                location, 
                region='tw-tz', 
                safesearch='strict', # <--- é—œéµä¿®æ”¹ï¼šå¼·åˆ¶é–‹å•Ÿå®‰å…¨æœå°‹
                timelimit='y',       # <--- å»ºè­°åŠ å…¥ï¼šåªæ‰¾ 'y' (éå»ä¸€å¹´) çš„è³‡æ–™
                max_results=3
            )
            
            for r in ddgs_gen:
                # results.append(f"æ¨™é¡Œ: {r['title']}\nç¶²å€: {r['href']}\næ‘˜è¦: {r['body']}")
                urls.append(r['href'])
    except Exception as e:
        print(f"âš ï¸ æœå°‹ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    if not urls:
        print("âŒ è­¦å‘Šï¼šæœå°‹çµæœç‚ºç©ºï¼è«‹æª¢æŸ¥é—œéµå­—æ˜¯å¦æ­£ç¢ºã€‚")
        
    # results = "\n\n".join(results)
    # print(results)
    # urls = "\n".join(urls)
    print("----------url--------")
    print(urls)

    return urls



def parse_attractions_from_url(urls, location):
    url0 = urls[0]
    url1 = urls[1]
    url2 = urls[2]

    client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
    model_id = "gemini-3-flash-preview"

    tools = [
    {"url_context": {}},
    ]

    prompt = f"""
        è«‹é–±è®€ä¸¦åˆ†æä»¥ä¸‹ç¶²å€å…§å®¹ï¼š
        {url0}, {url1}, {url2}

        # ä»»å‹™
        1. æå–æ‰€æœ‰é—œæ–¼ã€Œ{location}ã€çš„æ—…éŠæ™¯é»ã€‚
        2. **å»é‡è™•ç†**ï¼šç›¸åŒæ™¯é»åƒ…ä¿ç•™ä¸€å€‹ã€‚
        3. **æè¿°ç”Ÿæˆ**ï¼šåƒè€ƒç¶²é ä¸­çš„ä»‹ç´¹ï¼Œç‚ºæ¯å€‹æ™¯é»æ’°å¯«ä¸€æ®µ 40 å­—åˆ° 50å­—ã€ç”Ÿå‹•ä¸”å…·å¸å¼•åŠ›çš„æè¿°ã€‚
        
        # è¼¸å‡ºæ ¼å¼ (åš´æ ¼è¦æ±‚ä½¿ç”¨ JSON)
        è«‹å›å‚³ä¸€å€‹ JSON æ ¼å¼çš„åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
        - "city": åŸå¸‚åç¨± (å­—ä¸²ï¼Œä¾‹å¦‚ï¼š"{location}")
        - "attraction": æ™¯é»åç¨± (å­—ä¸²)
        - "description": æ™¯é»æè¿° (å­—ä¸²)

        ç¯„ä¾‹ï¼š
        [
            {{"city": "{location}", "attraction": "æ™¯é» A", "description": "æè¿° A..."}},
            {{"city": "{location}", "attraction": "æ™¯é» B", "description": "æè¿° B..."}}
        ]

    """

    response = client.models.generate_content(
        model=model_id,
        contents=prompt,
        config=GenerateContentConfig(
            tools=tools,
        )
    )

    raw_data = response.candidates[0].content.parts[0].text
    match = re.search(r"```(?:json)?\s*([\s\S]*?)\s*```", raw_data)

    if match:
        json_content = match.group(1)
        data = json.loads(json_content)
    else:
        # å¦‚æœæ²’æŠ“åˆ°æ¨™ç±¤ï¼Œå°±å˜—è©¦ç›´æ¥è§£æ
        data = json.loads(raw_data)
    
    return data

def search_attraction_imgs(ai_gen_data):
    total_result = []
    attractions = [item.get('attraction')for item in ai_gen_data]
    
    for attraction in attractions:
        # åˆå§‹åŒ–çµæœå­—å…¸
        attraction_data = {
            "name": attraction,
            "images": [],
        }

        # ä½¿ç”¨ context manager è‡ªå‹•è™•ç†é€£ç·š
        with DDGS() as ddgs:
            # ---------------------------------------------------------
            # 2. æœå°‹åœ–ç‰‡ (åŠ å…¥ç‰ˆæ¬Šéæ¿¾)
            # ---------------------------------------------------------
            try:
                # åŠ å…¥ license åƒæ•¸
                # license='Public' -> å…¬çœ¾é ˜åŸŸ (æœ€å®‰å…¨ï¼Œåƒ CC0)
                # license='Share'  -> å…è¨±åˆ†äº« (é€šå¸¸éœ€è¦æ¨™ç¤ºå‡ºè™•)
                # license='Modify' -> å…è¨±ä¿®æ”¹
                
                images_results = list(ddgs.images(
                    attraction, 
                    max_results=3, 
                    safesearch='on',
                    license='Public'  # <--- é—œéµä¿®æ”¹åœ¨é€™è£¡ï¼
                ))
                
                for img in images_results:
                    attraction_data["images"].append({
                        "url": img.get("image"),
                        "source": img.get("url") # æœ€å¥½ä¿ç•™åŸå§‹ç¶²é é€£çµï¼Œä»¥å‚™ä¸æ™‚ä¹‹éœ€
                    })
                    
            except Exception as e:
                print(f"   âŒ åœ–ç‰‡æœå°‹éŒ¯èª¤: {e}")

        total_result.append(attraction_data)

    return total_result

def combine_data(ai_gen_data, img_data):
    # å°‡ img_data è½‰æ›æˆä»¥åç¨±ç‚º Key çš„å­—å…¸ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
    # æ ¼å¼ï¼š{'æ—¥æ¸…æ¯éºµåšç‰©é¤¨': [{'url':...}, {...}], ...}
    img_dict = {item['name']: item['images'] for item in img_data}

    combine_data = []

    for item in ai_gen_data:
        attraction_name = item.get('attraction')
        images = img_dict.get(attraction_name, [])
        

        data = {
            'city': item.get('city'),
            'attraction': item.get('attraction'),
            'description': item.get('description'),
            'images': images
        }
        combine_data.append(data)


    return combine_data


def write_into_db(combine_data):
    # å› æ¸¬è©¦ç”¨æ²’æœ‰ç”¨Fastapiæ‰€ä»¥å…ˆä¸ç”¨Depends
    conn = POOL.connection()

    try:
        # é–‹å•Ÿäº‹å‹™ (æœ‰äº›é€£ç·šæ± é è¨­æœƒå¹«ä½ åšï¼Œä½†æ‰‹å‹•æ›´ä¿éšª)
        cur = conn.cursor()

        for item in combine_data:
            # æ’å…¥å–®å€‹æ™¯é»çš„è³‡è¨Š
            dest_sql = "INSERT INTO destinations(city_name, place_name, description) VALUES(%s, %s, %s)"
            cur.execute(dest_sql, (item.get('city'), item.get('attraction'), item.get('description')))
            
            # å–å¾—å‰›æ’å…¥çš„æ™¯é»id
            dest_id = cur.lastrowid
            
            # 3. é‡å°è©²æ™¯é»çš„æ‰€æœ‰åœ–ç‰‡ï¼Œä½¿ç”¨ executemany
            images = item.get('images', [])
            if images:
                img_sql = "INSERT INTO destination_photos(destination_id, photo_url, source_url) VALUES(%s, %s, %s)"
                img_insert_data = [(dest_id, img.get('url'), img.get('source')) for img in images]
                cur.executemany(img_sql, img_insert_data)
        
        conn.commit()
        print(f"æˆåŠŸå¯«å…¥ {len(combine_data)} ç­†æ™¯é»åŠåœ–ç‰‡")
    except pymysql.MySQLError as e:
        conn.rollback() 
        print(f"Database error: {e}ï¼Œæ™¯é»è³‡æ–™å¯«å…¥DBå¤±æ•—")
        raise HTTPException(status_code=500, detail="æ™¯é»è³‡æ–™å¯«å…¥DBå¤±æ•—")
    finally:
        conn.close()
        

location = "é¦–çˆ¾"
urls = search_with_duckduckgo(f"{location} æ—…éŠéŠè¨˜ å¿…å»æ™¯é»")
ai_gen_data = parse_attractions_from_url(urls, location)
img_data = search_attraction_imgs(ai_gen_data)
combine_data = combine_data(ai_gen_data, img_data)
write_into_db(combine_data)



# ai_gen_data = [
#   {
#     "city": "å¤§é˜ª",
#     "attraction": "æ—¥æ¸…æ¯éºµåšç‰©é¤¨ å¤§é˜ªæ± ç”°",
#     "description": "èµ°é€²å……æ»¿å‰µæ„çš„æ³¡éºµä¸–ç•Œï¼Œé™¤äº†äº†è§£æ­·å²ï¼Œé‚„èƒ½è¦ªè‡ªå½©ç¹ªæ¯èº«ã€æŒ‘é¸é…æ–™ï¼Œè£½ä½œå‡ºç¨ä¸€ç„¡äºŒçš„å°ˆå±¬æ¯éºµï¼Œæ˜¯æ¥µå…·è¶£å‘³çš„DIYé«”é©—ã€‚"
#   },
#   {
#     "city": "å¤§é˜ª",
#     "attraction": "LaLaport é–€çœŸ / ä¸‰äº• Outlet Park",
#     "description": "çµåˆäº† Outlet èˆ‡è³¼ç‰©ä¸­å¿ƒçš„å…¨æ–°åœ°æ¨™ï¼Œé›†åˆçœ¾å¤šçŸ¥åå“ç‰Œèˆ‡é»‘é–€å¸‚å ´ç¾é£Ÿã€‚å¯¬æ•å¥½é€›çš„ç©ºé–“èƒ½ä¸€æ¬¡è³¼è¶³æ‰€éœ€ï¼Œæ˜¯å¤§é˜ªè¿‘æœŸè¡€æ‹šå‹åœ°ã€‚"
#   }
# ]

# img_data = [
#     {
#         'name': 'æ—¥æ¸…æ¯éºµåšç‰©é¤¨ å¤§é˜ªæ± ç”°', 
#         'images': 
#             [
#                 {'url': 'https://res-4.cloudinary.com/jnto/image/upload/w_2064,h_1300,c_fill,f_auto,fl_lossy,q_auto/v1645167602/osaka/M_00142_001', 'source': 'https://www.japan.travel/hk/spot/1081/'},
#                 {'url': 'https://osaka-info.jp/spot/images/47fed0c6d01ddcfaf69c71c4f53d68eb5530e614.jpg', 'source': 'https://cn.osaka-info.jp/spot/gastronomy-cupnoodle/'}, 
#                 {'url': 'https://www.kiri-san.com/wp-content/uploads/2018/12/æ± ç”°é§…_8244.jpg', 'source': 'https://www.kiri-san.com/post/25228'}
#             ]
#     }, 
#     {
#         'name': 'LaLaport é–€çœŸ / ä¸‰äº• Outlet Park', 
#         'images': 
#         [
#             {'url': 'https://assets.funliday.com/posts/wp-content/uploads/2023/07/18120401/ã‚‰ã‚‰ã½ãƒ¼ã¨å¤§é˜ªé–€çœŸ.jpg', 'source': 'https://www.funliday.com/posts/japan-info-shopping-osaka-outlet-kodoma/'}, 
#             {'url': 'https://i.ytimg.com/vi/__iYy0ejuf0/maxresdefault.jpg', 'source': 'https://www.youtube.com/watch?v=__iYy0ejuf0'}, 
#             {'url': 'https://carlming.net/wp-content/uploads/2024/04/20231013-20231013_170120.jpg', 'source': 'https://carlming.net/57622'}
#         ]
#     }
# ]



# def extract_json_data(ai_response):
#     # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼å°‹æ‰¾ [ ... ] æ ¼å¼çš„å…§å®¹
#     # re.DOTALL ç¢ºä¿å¯ä»¥åŒ¹é…å¤šè¡Œæ–‡å­—
#     match = re.search(r'\[.*\]', ai_response, re.DOTALL)
    
#     if match:
#         json_str = match.group(0)
#         try:
#             # è½‰æ›æˆ Python çš„ List
#             data_list = json.loads(json_str)
#             return data_list
#         except json.JSONDecodeError as e:
#             print(f"JSON è§£æå¤±æ•—: {e}")
#     return None

# r'\[.*\]'ï¼š
# \[ï¼šå«ç¨‹å¼å»æ‰¾å·¦ä¸­æ‹¬è™Ÿ [ã€‚å› ç‚º [ åœ¨æ­£å‰‡è¡¨é”å¼ä¸­æœ‰ç‰¹æ®Šæ„ç¾©ï¼Œæ‰€ä»¥å‰é¢è¦åŠ ä¸€å€‹åæ–œç·š \ å‘Šè¨´å®ƒã€Œæˆ‘è¦æ‰¾çš„å°±æ˜¯é€™å€‹ç¬¦è™Ÿã€ã€‚
# .*ï¼š. ä»£è¡¨ã€Œä»»ä½•å­—å…ƒã€ï¼Œ* ä»£è¡¨ã€Œä¸é™æ•¸é‡ã€ã€‚çµ„åˆèµ·ä¾†å°±æ˜¯ã€Œä¸­é–“ç®¡å®ƒå¯«ä»€éº¼éƒ½çµ¦æˆ‘åŒ…é€²ä¾†ã€ã€‚
# \]ï¼šå«ç¨‹å¼å»æ‰¾å³ä¸­æ‹¬è™Ÿ ]ã€‚
# re.DOTALLï¼š
# é è¨­æƒ…æ³ä¸‹ï¼Œæ­£å‰‡è¡¨é”å¼çœ‹åˆ°ã€Œæ›è¡Œã€å°±æœƒåœä½ã€‚
# åŠ ä¸Šé€™å€‹è¨­å®šå¾Œï¼Œå®ƒæœƒç„¡è¦–æ›è¡Œï¼ŒæŠŠæ•´å€‹å¤šè¡Œçš„ JSON å€å¡Šç•¶æˆä¸€å€‹é•·é•·çš„å­—ä¸²ã€‚
# æƒ³åƒä¸€ä¸‹ï¼š å®ƒå°±åƒæ˜¯åœ¨é›œäº‚çš„æˆ¿é–“è£¡ï¼ˆAI çš„å›è¦†ï¼‰ï¼Œæ‰¾åˆ°ä¸€å€‹å¤§ç®±å­ï¼ˆä»¥ [ é–‹å§‹ã€ä»¥ ] çµæŸçš„åœ°æ–¹ï¼‰ã€‚