import os
from google import genai
from google.genai import types
from google.genai.types import Tool, GenerateContentConfig
from dotenv import load_dotenv
load_dotenv()
import json
from ddgs import DDGS
from services.db_service import *
import re
import json
import time, random


def get_travel_blog_urls(location):
    # ğŸ” Debug: å…ˆå°å‡ºä¾†çœ‹çœ‹ï¼Œç¢ºå®šçœŸçš„æœ‰å‚³å°é—œéµå­—é€²å»
    target = f"{location} æ—…éŠéŠè¨˜ å¿…å»æ™¯é»"
    print(f"ğŸ•µï¸ æ­£åœ¨å‘ DuckDuckGo æŸ¥è©¢é—œéµå­—ï¼š[{target}]") 
    urls = []
    excluded_domains = ["googleusercontent.com", "facebook.com", "youtube.com", "591.com", "shopee", "wikipedia"]
    travel_keywords = ["éŠè¨˜", "æ™¯é»", "æ¨è–¦", "è¡Œç¨‹", "æ”»ç•¥", "æ‡¶äººåŒ…", "æ‰“å¡"]
    try:
        with DDGS() as ddgs:
            # ==========================================
            # ğŸ›¡ï¸ åŠ å…¥ safesearch='strict' (åš´æ ¼éæ¿¾æˆäºº/åƒåœ¾å…§å®¹)
            # ğŸ›¡ï¸ ç¢ºèª region='tw-tz' (é–å®šå°ç£ç¹é«”ä¸­æ–‡çµæœ)
            # ==========================================
            ddgs_gen = ddgs.text(
                target, 
                region='tw-tz', 
                safesearch='strict', # <--- é—œéµä¿®æ”¹ï¼šå¼·åˆ¶é–‹å•Ÿå®‰å…¨æœå°‹
                timelimit='y',       # <--- å»ºè­°åŠ å…¥ï¼šåªæ‰¾ 'y' (éå»ä¸€å¹´) çš„è³‡æ–™
                max_results=10
            )
            for r in ddgs_gen:
                href = r['href'].lower()
                title = r['title']
                body = r['body']
                print(f"{r['href']}\n\n{title}\n\n{body}\n\n")

                # ç§»é™¤æ¨™é¡Œèˆ‡æ‘˜è¦ä¸­çš„æ‰€æœ‰ç©ºç™½ï¼ˆåŒ…æ‹¬å…¨å½¢ã€åŠå½¢ã€æ›è¡Œï¼‰
                clean_title = re.sub(r'\s+', '', title)
                clean_body = re.sub(r'\s+', '', body)
                
                # éæ¿¾æœå°‹çµæœ
                is_valid_url = not any(domain in href for domain in excluded_domains)
                is_relevant = any(key in clean_title or key in clean_body for key in travel_keywords)
                correct_location = (location in clean_title) or (location in clean_body)
                
                if is_valid_url and is_relevant and correct_location:
                    urls.append(r['href'])
    except Exception as e:
        print(f"âš ï¸ æœå°‹ç™¼ç”ŸéŒ¯èª¤: {e}")
    
    if not urls:
        print("âŒ è­¦å‘Šï¼šæœå°‹çµæœç‚ºç©ºï¼è«‹æª¢æŸ¥é—œéµå­—æ˜¯å¦æ­£ç¢ºã€‚")
        raise HTTPException(status_code=500, detail="æ²’æœ‰ç¬¦åˆè¦æ±‚çš„ç¶²å€")
    print(f"ç¸½å…±æœå°‹{len(ddgs_gen)}ç­†çµæœï¼Œæœ‰{len(urls)}ç­†ç¬¦åˆè¦æ±‚")
    print(urls[:3])
    return urls[:3]


def extract_spots_from_urls(urls, location):
    url0 = urls[0]
    url1 = urls[1]
    url2 = urls[2]

    client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
    model_id = "gemini-2.5-flash"
    # model_id = "gemini-3-flash-preview"

    tools = [
    {"url_context": {}},
    ]

    prompt = f"""
        è«‹é–±è®€ä¸¦åˆ†æä»¥ä¸‹ç¶²å€å…§å®¹ï¼š
        {url0}, {url1}, {url2}

        # ä»»å‹™
        1. æå–æ‰€æœ‰é—œæ–¼ã€Œ{location}ã€çš„æ—…éŠæ™¯é»ã€‚
        2. **å»é‡è™•ç†**ï¼šç›¸åŒæ™¯é»åƒ…ä¿ç•™ä¸€å€‹ã€‚
        3. **æè¿°ç”Ÿæˆ**ï¼šåƒè€ƒç¶²é ä¸­çš„ä»‹ç´¹ï¼Œç‚ºæ¯å€‹æ™¯é»æ’°å¯«ä¸€æ®µ 40 å­—å·¦å³ã€ç”Ÿå‹•ä¸”å…·å¸å¼•åŠ›çš„æè¿°ã€‚
        
        # è«‹å›å‚³ä¸€å€‹ JSON æ ¼å¼çš„åˆ—è¡¨ï¼Œæ¯å€‹å…ƒç´ åŒ…å«ä»¥ä¸‹æ¬„ä½ï¼š
        - "city": æ™¯é»æ‰€åœ¨çš„å…·é«”è¡Œæ”¿åŸå¸‚/ç¸£åç¨± (å­—ä¸²ï¼Œè«‹å¾ç¶²é å…§å®¹åˆ†æå¾—å‡º)
        - "attraction": æ™¯é»åç¨± (å­—ä¸²)
        - "description": æ™¯é»æè¿° (å­—ä¸²)
        - "geo_tags": å¾å¤§åˆ°å°çš„åœ°ç†æ¨™ç±¤å­—ä¸²ï¼Œç”¨é€—è™Ÿéš”é–‹ (ä¾‹å¦‚ï¼šåœ‹å®¶,å·/çœ,åŸå¸‚)

        ç¯„ä¾‹ (åƒ…ä¾›æ ¼å¼åƒè€ƒï¼Œè«‹æ ¹æ“šå¯¦éš›æœå°‹å…§å®¹èª¿æ•´)ï¼š
        [
            {{"city": "å…·é«”åŸå¸‚A", "attraction": "æ™¯é» A", "description": "æè¿° A...", "geo_tags": "åœ‹å®¶,å€åŸŸ,å…·é«”åŸå¸‚A"}},
            {{"city": "å…·é«”åŸå¸‚B", "attraction": "æ™¯é» B", "description": "æè¿° B...", "geo_tags": "åœ‹å®¶,å€åŸŸ,å…·é«”åŸå¸‚B"}}
        ]

        # è¦å‰‡
        1. ç›´æ¥ä»¥ [ é–‹é ­ï¼Œä¸¦ä»¥ ] çµå°¾ã€‚
        2. ä¸è¦ä½¿ç”¨ Markdown çš„ç¨‹å¼ç¢¼å€å¡Šæ¨™ç±¤ï¼ˆå¦‚ ```jsonï¼‰ã€‚
        3. å¦‚æœæ‰€æœ‰ç¶²å€éƒ½å¤±æ•ˆï¼Œè«‹å›å‚³ç©ºé™£åˆ— []ã€‚

    """
    try:
        print("--------------------Geminiæº–å‚™é–‹å§‹è·‘--------------------------")
        response = client.models.generate_content(
            model=model_id,
            contents=prompt,
            config=GenerateContentConfig(
                tools=tools,
            )
        )

        for part in reversed(response.candidates[0].content.parts):
            print(part)
            if not part.text:
                continue
            
            raw_data = part.text
        
            print(raw_data)
            print("-----------------------æˆåŠŸå•¦ï¼ï¼ï¼-------------------------------")
            # [[\s\S]*] ä»£è¡¨å¾ç¬¬ä¸€å€‹ [ åŒ¹é…åˆ°æœ€å¾Œä¸€å€‹ ]ï¼ŒåŒ…å«æ›è¡Œ
            match = re.search(r'\[[\s\S]*\]', raw_data)

            if match:
                json_content = match.group(0)
                json_content = json_content.replace('```json', '').replace('```', '')
                data = json.loads(json_content)
            else:
                # å¦‚æœæ²’æŠ“åˆ°æ¨™ç±¤ï¼Œå°±å˜—è©¦ç›´æ¥è§£æ
                data = json.loads(raw_data)
            print(f"ç¸½å…±æœ‰ {len(data)} ç­†æ™¯é»")
            return data
        
        return []
    except json.JSONDecodeError as e:
        print(f"âŒ JSON è§£æå¤±æ•—ï¼ŒGemini å›å‚³æ ¼å¼ä¸æ­£ç¢º: {e}")
        raise HTTPException(status_code=500, detail=f"JSON è§£æå¤±æ•—ï¼ŒGemini å›å‚³æ ¼å¼ä¸æ­£ç¢º")
    except Exception as e:
        print(f"ğŸš¨ Geminiç™¼ç”Ÿéé æœŸéŒ¯èª¤: {type(e).__name__}: {e}")
        raise HTTPException(status_code=500, detail=f"Geminiç™¼ç”Ÿæœªé æœŸéŒ¯èª¤ï¼Œè«‹é‡è©¦")
    
def fetch_attraction_images(ai_gen_data):
    total_result = []
    attractions = [item.get('attraction')for item in ai_gen_data]
    
    for attraction in attractions:
        # åˆå§‹åŒ–çµæœå­—å…¸
        attraction_data = {
            "name": attraction,
            "images": [],
        }

        # è¨­å®šretryï¼Œé¿å…æŠ“åœ–å¤±æ•—
        max_retries = 3
        retry_delay = 1

        for i in range(max_retries):
            
            # ä½¿ç”¨ context manager è‡ªå‹•è™•ç†é€£ç·š
            with DDGS() as ddgs:
                # ---------------------------------------------------------
                # 2. æœå°‹åœ–ç‰‡ (åŠ å…¥ç‰ˆæ¬Šéæ¿¾)
                # ---------------------------------------------------------
                try:
                    # åŠ å…¥ license åƒæ•¸
                    # license='Public' -> å…¬çœ¾é ˜åŸŸ (æœ€å®‰å…¨ï¼Œåƒ CC0)
                    # license='Share'  -> å…è¨±åˆ†äº« (é€šå¸¸éœ€è¦æ¨™ç¤ºå‡ºè™•)
                    # license='Modify' -> å…è¨±ä¿®æ”¹
                    
                    images_results = list(ddgs.images(
                        attraction, 
                        max_results=3, 
                        safesearch='on',
                        license='Public'  # <--- é—œéµä¿®æ”¹åœ¨é€™è£¡ï¼
                    ))
                    if images_results:
                        for img in images_results:
                            attraction_data["images"].append({
                                "url": img.get("image"),
                                "source": img.get("url") # æœ€å¥½ä¿ç•™åŸå§‹ç¶²é é€£çµï¼Œä»¥å‚™ä¸æ™‚ä¹‹éœ€
                            })
                        break # æ‰¾åˆ°åœ–ç‰‡ï¼Œæ›ä¸‹ä¸€å€‹æ™¯é»
                    else:
                        raise Exception("æ‰¾ä¸åˆ°åœ–ç‰‡")
                        
                except Exception as e:
                    print(f"   âš ï¸ ç¬¬ {i + 1} æ¬¡å˜—æŠ“å–å–åœ–ç‰‡å¤±æ•— ({attraction}): {e}")
                    if i < max_retries:
                        # æŒ‡æ•¸é€€é¿ + éš¨æ©ŸæŠ–å‹•ï¼Œé¿å…è¢«ä¼ºæœå™¨åµæ¸¬ç‚ºæ©Ÿå™¨äºº
                        sleep_time = (retry_delay * 2 ** i) + random.uniform(0, 1)
                        time.sleep(sleep_time)
                    else:
                        print(f"âŒ {attraction}åœ–ç‰‡æœå°‹éŒ¯èª¤: {e}")

        total_result.append(attraction_data)
        
        # # æ™¯é»ä¹‹é–“ç¨å¾®åœé “ï¼Œé¿å…è¢«å°é–ï¼Œä¹‹å¾Œæœ‰éœ€è¦å†é–‹å•Ÿ
        # time.sleep(0.5)

    return total_result

def integrate_spot_results(location, ai_gen_data, img_data):
    # å°‡ img_data è½‰æ›æˆä»¥åç¨±ç‚º Key çš„å­—å…¸ï¼Œæ–¹ä¾¿æŸ¥æ‰¾
    # æ ¼å¼ï¼š{'æ—¥æ¸…æ¯éºµåšç‰©é¤¨': [{'url':...}, {...}], ...}
    img_dict = {item['name']: item['images'] for item in img_data}

    result = []

    for item in ai_gen_data:
        attraction_name = item.get('attraction')
        images = img_dict.get(attraction_name, [])
        

        data = {
            'input_region': location,
            'city': item.get('city'),
            'attraction': item.get('attraction'),
            'description': item.get('description'),
            'geo_tags': item.get('geo_tags'),
            'images': images
        }
        result.append(data)


    return result



def run_web_scraping_workflow(location):
    urls = get_travel_blog_urls(location)
    print("4")
    ai_gen_data = extract_spots_from_urls(urls, location)
    print("5")
    img_data = fetch_attraction_images(ai_gen_data)
    print("6")
    result = integrate_spot_results(location, ai_gen_data, img_data)

    return result

